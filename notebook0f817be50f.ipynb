{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959ca15b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 7.861208,
     "end_time": "2022-01-30T05:59:50.466088",
     "exception": false,
     "start_time": "2022-01-30T05:59:42.604880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words: [('the', 112780), ('a', 61522), ('and', 60362), ('of', 56663), ('to', 52397), ('is', 40621), ('in', 34059), ('I', 26135), ('that', 25365), ('this', 23018)]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "from collections import Counter\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "filename = \"./Train.csv\"\n",
    "df = pd.read_csv(filename, usecols=['text', 'label'], dtype={'text': 'str', 'label': 'int64'})\n",
    "dfX = df.loc[:, 'text']\n",
    "dfY = df.loc[:, 'label']\n",
    "dfX = dfX[:10000]\n",
    "dfY = dfY[:10000]\n",
    "\n",
    "vocabulary = Counter()\n",
    "for title in dfX:\n",
    "    words = title.split()\n",
    "    validWords = filter(lambda x: len(x) <= 14, words)\n",
    "    vocabulary.update(validWords)\n",
    "\n",
    "# Truncate vocabulary\n",
    "vocab_size = 5000\n",
    "truncatedVocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]\n",
    "\n",
    "# Print out the 10 most common words and the number of times they occur\n",
    "print(\"Most common words:\", vocabulary.most_common()[:10])\n",
    "\n",
    "# Convert words to tensor\n",
    "words = tf.constant(truncatedVocabulary)\n",
    "\n",
    "# Assign each word an ID\n",
    "word_ids = tf.range(len(truncatedVocabulary), dtype=tf.int64)\n",
    "\n",
    "# Create KeyValueTensor\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "\n",
    "# Create lookup table\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
    "\n",
    "# Test the lookup table\n",
    "#testArr = \"China and Iraq are in the dataset\".split()\n",
    "#testRes = table.lookup(tf.constant(testArr))\n",
    "#print(\"Test result:\", testRes)\n",
    "\n",
    "print(\"done\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18684f9",
   "metadata": {
    "papermill": {
     "duration": 15.761671,
     "end_time": "2022-01-30T06:00:06.234088",
     "exception": false,
     "start_time": "2022-01-30T05:59:50.472417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2470)\n",
      "(10000,)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Convert sentences to arrays of word ids\n",
    "data = []\n",
    "for title in dfX:\n",
    "    sample = tf.strings.split(title)\n",
    "    processed = table.lookup(sample)\n",
    "    data.append(processed.numpy())\n",
    "\n",
    "# Create a ragged tensor and then convert it to a padded dense tensor\n",
    "ragged = tf.ragged.constant(data)\n",
    "ragged = ragged.to_tensor(default_value=0)\n",
    "\n",
    "# Make dataset\n",
    "features = tf.constant(ragged)\n",
    "print(features.shape)\n",
    "labels = tf.constant(dfY)\n",
    "print(labels.shape)\n",
    "\n",
    "train_set = tf.data.Dataset.from_tensor_slices((features, labels)).batch(32).prefetch(1)\n",
    "#print(train_set)\n",
    "\n",
    "#print(next(train_set.batch(32).as_numpy_iterator())[0][0])\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53bb8954",
   "metadata": {
    "papermill": {
     "duration": 2651.367693,
     "end_time": "2022-01-30T06:44:17.608410",
     "exception": false,
     "start_time": "2022-01-30T06:00:06.240717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/313 [==============================] - 1234s 4s/step - loss: 0.6064 - accuracy: 0.6626\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 1231s 4s/step - loss: 0.4059 - accuracy: 0.8152\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 1225s 4s/step - loss: 0.2291 - accuracy: 0.9099\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 1241s 4s/step - loss: 0.1460 - accuracy: 0.9459\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 1230s 4s/step - loss: 0.1385 - accuracy: 0.9441\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape=[None], mask_zero=True),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, batch_size=32, epochs=5)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fa8c2e9",
   "metadata": {
    "papermill": {
     "duration": 1.021375,
     "end_time": "2022-01-30T06:44:18.867259",
     "exception": false,
     "start_time": "2022-01-30T06:44:17.845884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12612972]]\n",
      "Review is positive: [False]\n"
     ]
    }
   ],
   "source": [
    "testArr = tf.strings.split(\"Not sure if I understood the premise at all. It was sort of weird. Probably won't see this again.\")\n",
    "test = table.lookup(testArr)\n",
    "zero_padding = tf.zeros(tf.shape(features)[1] - tf.shape(test)[0], dtype=tf.int64)\n",
    "a_padded = tf.concat([test, zero_padding],0)\n",
    "a_padded = a_padded.numpy().reshape(1,-1);\n",
    "#print(\"Prediction input: \", a_padded)\n",
    "prediction = model.predict(a_padded)\n",
    "print(prediction)\n",
    "print(\"Review is positive:\", prediction[0] > 0.94)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d643d3",
   "metadata": {
    "papermill": {
     "duration": 0.241681,
     "end_time": "2022-01-30T06:44:19.345135",
     "exception": false,
     "start_time": "2022-01-30T06:44:19.103454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/JP/Desktop/Projects/imdb_sentiment/saved_models/latest\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:/Users/JP/Desktop/Projects/imdb_sentiment/saved_models/latest\\assets\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002ACEDE47B80> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x000002ACEDE47EB0> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save('C:/Users/JP/Desktop/Projects/imdb_sentiment/saved_models/latest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e47af6-331c-46dc-b43b-4eda2281dc2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2689.792157,
   "end_time": "2022-01-30T06:44:22.838914",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-30T05:59:33.046757",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
