{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "959ca15b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 7.861208,
     "end_time": "2022-01-30T05:59:50.466088",
     "exception": false,
     "start_time": "2022-01-30T05:59:42.604880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "import os\n",
    "from collections import Counter\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# Files\n",
    "trainingDataFilename = \"data/Train.csv\"\n",
    "\n",
    "# Read training data from CSV\n",
    "df = pd.read_csv(trainingDataFilename, usecols=['text', 'label'], dtype={'text': 'str', 'label': 'int64'})\n",
    "dfX = df.loc[:, 'text']\n",
    "dfY = df.loc[:, 'label']\n",
    "dfX = dfX[:10000]\n",
    "dfY = dfY[:10000]\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary = Counter()\n",
    "for title in dfX:\n",
    "    words = title.split()\n",
    "    validWords = filter(lambda x: len(x) <= 14, words)\n",
    "    vocabulary.update(validWords)\n",
    "\n",
    "# Truncate vocabulary\n",
    "vocab_size = 5000\n",
    "truncatedVocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]\n",
    "\n",
    "# Print out the 5 most common words and the number of times they occur\n",
    "#print(\"5 most common words:\", vocabulary.most_common()[:5])\n",
    "\n",
    "# Convert words to tensor\n",
    "words = tf.constant(truncatedVocabulary)\n",
    "\n",
    "# Assign each word an ID\n",
    "word_ids = tf.range(len(truncatedVocabulary), dtype=tf.int64)\n",
    "\n",
    "# Create KeyValueTensor\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "\n",
    "# Create lookup table\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
    "\n",
    "# Test the lookup table\n",
    "#testArr = \"China and Iraq are in the dataset\".split()\n",
    "#testRes = table.lookup(tf.constant(testArr))\n",
    "#print(\"Test result:\", testRes)\n",
    "\n",
    "print(\"done\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a5493a-562a-4254-a47d-9dc9d19159fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabulary to disk\n",
    "\n",
    "numpy_data = np.array([words, word_ids])\n",
    "df = pd.DataFrame(data=numpy_data)\n",
    "print('Vocabulary shape:', df.shape)\n",
    "print('Saving vocabulary to latest_vocabulary.csv.')\n",
    "df.to_csv(vocabularyFilename, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c18684f9",
   "metadata": {
    "papermill": {
     "duration": 15.761671,
     "end_time": "2022-01-30T06:00:06.234088",
     "exception": false,
     "start_time": "2022-01-30T05:59:50.472417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2470)\n",
      "(10000,)\n",
      "Length of longest title (featureLen) which will be used for the max padding of zeroes later: tf.Tensor(2470, shape=(), dtype=int32)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Convert sentences to arrays of word ids\n",
    "\n",
    "data = []\n",
    "for title in dfX:\n",
    "    sample = tf.strings.split(title)\n",
    "    processed = table.lookup(sample)\n",
    "    data.append(processed.numpy())\n",
    "\n",
    "# Create a ragged tensor and then convert it to a padded dense tensor\n",
    "ragged = tf.ragged.constant(data)\n",
    "ragged = ragged.to_tensor(default_value=0)\n",
    "\n",
    "# Make dataset\n",
    "features = tf.constant(ragged)\n",
    "print(features.shape)\n",
    "labels = tf.constant(dfY)\n",
    "print(labels.shape)\n",
    "\n",
    "# Find the largest feature vector length\n",
    "featureLen = tf.shape(features)[1]\n",
    "print('Length of longest entry (to be used for the max padding of zeroes later):', featureLen)\n",
    "\n",
    "# Convert to dataset\n",
    "train_set = tf.data.Dataset.from_tensor_slices((features, labels)).batch(32).prefetch(1)\n",
    "#print(next(train_set.batch(32).as_numpy_iterator())[0][0])\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "189797c4-639d-4090-98b7-4a9e39b3bf65",
   "metadata": {
    "papermill": {
     "duration": 2651.367693,
     "end_time": "2022-01-30T06:44:17.608410",
     "exception": false,
     "start_time": "2022-01-30T06:00:06.240717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "\n",
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape=[None], mask_zero=True),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_set, batch_size=32, epochs=5)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcaec916-a30c-44c0-8224-8ff526ded448",
   "metadata": {
    "papermill": {
     "duration": 2651.367693,
     "end_time": "2022-01-30T06:44:17.608410",
     "exception": false,
     "start_time": "2022-01-30T06:00:06.240717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/313 [==============================] - 1234s 4s/step - loss: 0.6064 - accuracy: 0.6626\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 1231s 4s/step - loss: 0.4059 - accuracy: 0.8152\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 1225s 4s/step - loss: 0.2291 - accuracy: 0.9099\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 1241s 4s/step - loss: 0.1460 - accuracy: 0.9459\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 1230s 4s/step - loss: 0.1385 - accuracy: 0.9441\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk\n",
    "\n",
    "model.save(\"saved_models/latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fa8c2e9",
   "metadata": {
    "papermill": {
     "duration": 1.021375,
     "end_time": "2022-01-30T06:44:18.867259",
     "exception": false,
     "start_time": "2022-01-30T06:44:17.845884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12612972]]\n",
      "Review is positive: [False]\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "\n",
    "testArr = tf.strings.split(\"Not sure if I understood the premise at all. It was sort of weird. Probably won't see this again.\")\n",
    "test = table.lookup(testArr)\n",
    "zero_padding = tf.zeros(tf.shape(features)[1] - tf.shape(test)[0], dtype=tf.int64)\n",
    "padded = tf.concat([test, zero_padding],0)\n",
    "padded = a_padded.numpy().reshape(1,-1);\n",
    "prediction = model.predict(padded)\n",
    "print(prediction)\n",
    "print(\"Review is positive:\", prediction[0] > 0.94)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "gpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2689.792157,
   "end_time": "2022-01-30T06:44:22.838914",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-30T05:59:33.046757",
   "version": "2.3.3"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
